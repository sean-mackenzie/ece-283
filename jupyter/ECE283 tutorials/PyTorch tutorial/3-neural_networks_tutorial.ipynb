{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
    "``autograd`` to define models and differentiate them.\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n",
    "returns the ``output``.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    ".. figure:: /_static/img/mnist.png\n",
    "   :alt: convnet\n",
    "\n",
    "   convnet\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it\n",
    "through several layers one after the other, and then finally gives the\n",
    "output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Define the network\n",
    "------------------\n",
    "\n",
    "Let’s define this network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.uniform_(m.weight.data)\n",
    "\n",
    "net = Net()\n",
    "net.apply(weights_init)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd``.\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``\n",
    "\n",
    "Here, params[0] contains weights of conv1 layer, params[1] contains biases of conv1, params[2] contains weights of conv2 and so forth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n",
      "[Parameter containing:\n",
      "tensor([[[[0.0205, 0.9384, 0.3466, 0.7664, 0.0063],\n",
      "          [0.7058, 0.4754, 0.6675, 0.8995, 0.6439],\n",
      "          [0.8649, 0.0038, 0.7128, 0.8942, 0.9093],\n",
      "          [0.9220, 0.6844, 0.4362, 0.7938, 0.0239],\n",
      "          [0.1146, 0.4099, 0.9849, 0.0324, 0.5438]]],\n",
      "\n",
      "\n",
      "        [[[0.7051, 0.0842, 0.0021, 0.3583, 0.3168],\n",
      "          [0.9908, 0.3503, 0.4393, 0.8822, 0.0797],\n",
      "          [0.8535, 0.6691, 0.4817, 0.9033, 0.2888],\n",
      "          [0.4153, 0.4550, 0.1055, 0.7175, 0.0537],\n",
      "          [0.9289, 0.4551, 0.2094, 0.5698, 0.8031]]],\n",
      "\n",
      "\n",
      "        [[[0.3295, 0.9075, 0.0347, 0.9648, 0.5102],\n",
      "          [0.9384, 0.3054, 0.9173, 0.2792, 0.0808],\n",
      "          [0.5655, 0.3247, 0.7561, 0.5384, 0.8735],\n",
      "          [0.3973, 0.5634, 0.8398, 0.9353, 0.0523],\n",
      "          [0.2315, 0.1442, 0.2364, 0.9225, 0.7752]]],\n",
      "\n",
      "\n",
      "        [[[0.8214, 0.7198, 0.0970, 0.8359, 0.0248],\n",
      "          [0.3105, 0.0163, 0.3921, 0.6235, 0.3206],\n",
      "          [0.3374, 0.8789, 0.6040, 0.5299, 0.1286],\n",
      "          [0.5204, 0.7614, 0.0337, 0.1286, 0.3253],\n",
      "          [0.4077, 0.8176, 0.1337, 0.7566, 0.6300]]],\n",
      "\n",
      "\n",
      "        [[[0.5561, 0.1821, 0.7456, 0.3766, 0.6224],\n",
      "          [0.3892, 0.0155, 0.6511, 0.6320, 0.4890],\n",
      "          [0.9290, 0.2510, 0.6560, 0.7696, 0.7429],\n",
      "          [0.2086, 0.0445, 0.0741, 0.5469, 0.2316],\n",
      "          [0.6551, 0.8665, 0.8336, 0.5693, 0.1276]]],\n",
      "\n",
      "\n",
      "        [[[0.0180, 0.6471, 0.5350, 0.5214, 0.0117],\n",
      "          [0.6774, 0.9870, 0.1487, 0.9165, 0.5600],\n",
      "          [0.7796, 0.4859, 0.0039, 0.1896, 0.8056],\n",
      "          [0.7720, 0.4660, 0.4485, 0.9878, 0.5533],\n",
      "          [0.6098, 0.8991, 0.8294, 0.7345, 0.5547]]]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1306,  0.1941,  0.1980, -0.0032, -0.0960,  0.0852],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[8.9088e-01, 4.0014e-01, 3.3892e-01, 9.5067e-01, 4.0682e-01],\n",
      "          [6.4026e-01, 4.0766e-01, 6.4493e-01, 2.0412e-01, 9.0457e-01],\n",
      "          [8.1277e-02, 2.8299e-01, 8.1264e-01, 1.5612e-01, 6.0493e-01],\n",
      "          [2.0370e-01, 3.2693e-01, 2.2760e-02, 6.0434e-01, 6.8855e-01],\n",
      "          [5.0752e-02, 2.8436e-01, 7.0127e-01, 1.0278e-01, 1.3438e-01]],\n",
      "\n",
      "         [[2.0378e-02, 6.3225e-01, 4.8944e-01, 4.8692e-01, 9.1792e-01],\n",
      "          [6.7294e-01, 9.1222e-01, 1.8086e-01, 1.2018e-02, 6.5575e-01],\n",
      "          [6.4599e-01, 1.4801e-01, 3.1805e-02, 3.5495e-04, 3.3445e-02],\n",
      "          [8.6688e-01, 8.0840e-01, 7.3364e-01, 7.5670e-02, 4.7246e-01],\n",
      "          [9.9624e-01, 7.9667e-02, 7.5639e-01, 5.0623e-02, 5.7696e-01]],\n",
      "\n",
      "         [[1.2970e-02, 7.8405e-01, 1.5579e-01, 2.9370e-01, 3.8603e-01],\n",
      "          [6.0148e-01, 4.5397e-01, 5.4217e-01, 6.6851e-01, 3.5576e-01],\n",
      "          [7.1187e-02, 5.0868e-01, 3.8702e-01, 2.1907e-01, 3.8617e-01],\n",
      "          [9.7589e-01, 9.1265e-01, 9.2403e-01, 3.3341e-01, 1.6234e-01],\n",
      "          [9.2044e-01, 1.4897e-01, 3.4834e-01, 4.9407e-01, 1.7862e-01]],\n",
      "\n",
      "         [[4.0455e-01, 8.5696e-01, 1.5522e-01, 3.5355e-01, 3.0324e-03],\n",
      "          [5.5382e-01, 2.4417e-01, 1.5194e-01, 4.8702e-01, 6.3112e-01],\n",
      "          [2.5358e-01, 6.5711e-01, 3.5279e-01, 8.9548e-01, 1.2134e-01],\n",
      "          [4.7393e-01, 1.7812e-01, 4.0554e-01, 6.0207e-01, 7.8515e-01],\n",
      "          [6.3582e-01, 1.0038e-01, 2.9229e-01, 8.4254e-01, 5.8188e-01]],\n",
      "\n",
      "         [[6.7465e-01, 7.8994e-01, 9.7834e-01, 3.5988e-01, 6.8677e-01],\n",
      "          [1.1788e-01, 2.8110e-02, 2.5236e-01, 8.1234e-01, 6.6874e-02],\n",
      "          [5.7259e-01, 4.3680e-01, 5.9825e-01, 5.4710e-01, 9.6445e-01],\n",
      "          [4.1954e-01, 4.2035e-01, 4.9576e-01, 8.5916e-01, 8.5227e-01],\n",
      "          [6.6363e-01, 7.7149e-02, 5.3211e-01, 3.5921e-01, 6.8003e-02]],\n",
      "\n",
      "         [[2.6462e-01, 1.7819e-01, 9.6246e-01, 2.8071e-01, 2.5526e-01],\n",
      "          [3.3378e-01, 9.6152e-01, 2.6571e-01, 4.3406e-01, 3.9410e-01],\n",
      "          [8.5629e-01, 1.7774e-04, 8.4082e-01, 7.5895e-02, 7.0170e-02],\n",
      "          [3.9866e-01, 5.2702e-01, 3.2559e-01, 1.0870e-01, 5.8740e-01],\n",
      "          [3.6499e-01, 7.7131e-01, 3.2044e-01, 7.9160e-01, 7.5134e-02]]],\n",
      "\n",
      "\n",
      "        [[[1.6877e-01, 7.5122e-01, 6.4073e-02, 4.6137e-01, 6.1060e-01],\n",
      "          [5.5846e-01, 7.9548e-01, 8.4552e-01, 8.3989e-01, 8.1072e-01],\n",
      "          [2.3426e-01, 9.8444e-01, 3.7079e-01, 3.0442e-02, 4.1849e-01],\n",
      "          [4.2936e-01, 6.2268e-01, 2.0783e-01, 9.8643e-01, 7.0668e-01],\n",
      "          [9.5676e-01, 3.7383e-01, 4.8780e-01, 3.6607e-01, 2.2720e-01]],\n",
      "\n",
      "         [[4.0620e-01, 7.4445e-01, 7.1421e-01, 7.1035e-01, 3.2721e-01],\n",
      "          [1.1891e-01, 5.6095e-01, 2.4707e-01, 9.0813e-01, 3.7592e-01],\n",
      "          [1.7982e-01, 6.6714e-02, 6.5536e-01, 5.6443e-01, 4.6212e-01],\n",
      "          [5.5012e-01, 1.7124e-01, 1.0549e-01, 3.6191e-01, 2.9437e-01],\n",
      "          [5.7303e-01, 6.2039e-01, 8.0885e-01, 1.8307e-01, 3.6692e-01]],\n",
      "\n",
      "         [[1.6798e-01, 3.2915e-01, 6.9068e-01, 7.9259e-01, 5.9993e-01],\n",
      "          [8.3934e-01, 2.1085e-02, 3.3297e-01, 1.6155e-01, 1.4129e-01],\n",
      "          [3.0589e-01, 9.6848e-01, 7.0796e-01, 8.1612e-02, 8.6891e-01],\n",
      "          [5.4335e-01, 6.6647e-01, 2.5325e-01, 3.6722e-01, 8.6222e-01],\n",
      "          [7.0688e-01, 1.6288e-01, 3.3487e-01, 3.3679e-01, 4.4704e-02]],\n",
      "\n",
      "         [[4.0166e-01, 1.0718e-01, 5.4342e-01, 4.7635e-01, 7.2678e-02],\n",
      "          [7.0614e-03, 5.5794e-01, 3.3832e-01, 4.7080e-01, 9.2945e-01],\n",
      "          [1.2401e-01, 4.2244e-01, 6.6314e-01, 1.0837e-01, 5.1741e-01],\n",
      "          [7.6404e-01, 8.0182e-01, 9.6091e-01, 8.3554e-01, 5.4253e-01],\n",
      "          [7.3856e-01, 9.0789e-01, 4.3880e-01, 4.4642e-01, 5.1224e-01]],\n",
      "\n",
      "         [[9.3047e-01, 7.7031e-01, 6.9140e-01, 8.6246e-01, 3.1731e-01],\n",
      "          [5.8169e-01, 2.0325e-01, 4.2939e-01, 4.7555e-01, 4.4144e-01],\n",
      "          [6.1652e-01, 2.4722e-02, 6.9832e-01, 9.1497e-01, 2.4893e-01],\n",
      "          [8.7257e-01, 2.4605e-02, 5.1396e-01, 3.0458e-01, 2.5542e-01],\n",
      "          [7.8008e-03, 6.3860e-01, 5.4185e-01, 9.3537e-01, 1.0312e-01]],\n",
      "\n",
      "         [[4.9944e-01, 9.0249e-01, 9.0727e-01, 3.7010e-01, 9.8240e-01],\n",
      "          [2.4445e-01, 8.5791e-01, 1.2541e-01, 7.6938e-01, 3.0500e-01],\n",
      "          [2.0785e-01, 5.6636e-01, 7.1925e-02, 2.5498e-01, 3.3418e-01],\n",
      "          [8.3146e-01, 5.1178e-01, 8.2795e-01, 1.6003e-01, 3.7808e-01],\n",
      "          [7.3735e-01, 7.5471e-01, 5.4115e-01, 7.6582e-01, 5.4311e-01]]],\n",
      "\n",
      "\n",
      "        [[[7.2479e-01, 2.2075e-01, 1.4595e-01, 7.1129e-01, 8.6085e-01],\n",
      "          [6.3879e-02, 3.0594e-01, 7.5224e-01, 1.6912e-01, 2.2971e-01],\n",
      "          [7.2237e-02, 4.7207e-01, 5.8190e-01, 3.4638e-01, 8.6446e-01],\n",
      "          [1.9990e-01, 6.8468e-01, 1.4586e-01, 3.7332e-01, 7.4395e-01],\n",
      "          [2.8708e-01, 1.0806e-01, 7.9188e-01, 3.6795e-01, 2.6923e-01]],\n",
      "\n",
      "         [[1.0547e-01, 3.7752e-01, 7.3097e-01, 1.4830e-02, 2.4382e-01],\n",
      "          [3.5029e-01, 9.6025e-01, 7.3640e-01, 1.8143e-01, 1.5523e-01],\n",
      "          [2.4986e-01, 4.3749e-01, 4.6069e-01, 5.9465e-01, 7.8148e-01],\n",
      "          [2.6916e-01, 6.5604e-01, 6.2055e-01, 3.2699e-01, 6.1644e-01],\n",
      "          [2.2823e-01, 2.9513e-01, 7.8687e-01, 3.0839e-01, 3.1896e-01]],\n",
      "\n",
      "         [[1.3727e-01, 8.5733e-01, 5.6611e-01, 9.7459e-02, 7.0113e-01],\n",
      "          [2.2823e-01, 8.6137e-01, 1.4115e-01, 7.3414e-01, 7.3854e-01],\n",
      "          [4.2634e-01, 1.0723e-01, 2.4073e-01, 7.1285e-01, 5.1177e-01],\n",
      "          [2.3425e-01, 1.6650e-01, 3.8777e-01, 6.1683e-01, 9.1040e-01],\n",
      "          [3.9056e-01, 4.5947e-01, 4.8452e-01, 8.1102e-01, 6.6679e-01]],\n",
      "\n",
      "         [[7.9144e-01, 6.8612e-01, 2.5576e-01, 5.2488e-01, 1.7449e-01],\n",
      "          [4.8493e-01, 3.5051e-01, 2.3369e-01, 4.1210e-01, 5.2266e-02],\n",
      "          [3.5882e-01, 6.1279e-01, 3.9078e-01, 8.3686e-01, 1.7796e-02],\n",
      "          [6.0942e-01, 6.6985e-01, 9.1126e-01, 7.0699e-01, 9.7695e-01],\n",
      "          [8.6611e-01, 3.2386e-01, 9.9455e-01, 9.9997e-01, 8.8173e-01]],\n",
      "\n",
      "         [[1.5187e-01, 8.9837e-01, 7.1877e-01, 5.6738e-01, 1.8430e-01],\n",
      "          [3.4031e-01, 7.5249e-01, 5.9125e-02, 2.4982e-01, 2.6948e-02],\n",
      "          [4.7451e-01, 8.6864e-01, 3.3700e-01, 7.6993e-02, 6.8014e-01],\n",
      "          [8.7843e-01, 6.1086e-01, 7.8542e-02, 5.1969e-01, 8.5951e-01],\n",
      "          [8.0288e-01, 1.5463e-01, 4.7401e-01, 3.9985e-01, 6.8259e-01]],\n",
      "\n",
      "         [[5.0716e-01, 6.4810e-01, 1.5288e-01, 1.9669e-01, 6.1216e-01],\n",
      "          [5.7322e-02, 1.2046e-01, 3.6355e-01, 8.1669e-01, 8.7875e-01],\n",
      "          [2.6389e-01, 1.4275e-01, 6.7993e-01, 8.0844e-01, 3.0111e-01],\n",
      "          [7.7560e-01, 9.6235e-01, 9.6592e-01, 4.1911e-01, 7.2825e-01],\n",
      "          [1.3159e-03, 6.8673e-01, 7.3771e-01, 9.9925e-01, 8.4094e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1.5334e-01, 6.5955e-01, 1.9992e-01, 3.2929e-01, 8.7344e-02],\n",
      "          [5.2767e-01, 9.7602e-01, 1.8578e-01, 5.5855e-01, 8.5826e-01],\n",
      "          [1.3545e-03, 9.9526e-01, 8.4336e-01, 4.0523e-01, 1.0414e-01],\n",
      "          [3.7083e-01, 6.5926e-02, 4.4489e-01, 7.8094e-01, 4.7049e-01],\n",
      "          [1.8069e-01, 9.8895e-01, 9.1953e-01, 5.1471e-02, 8.2233e-01]],\n",
      "\n",
      "         [[1.5490e-02, 7.9026e-01, 8.0087e-01, 3.8744e-01, 7.6336e-01],\n",
      "          [3.2392e-01, 4.6356e-01, 5.8071e-01, 4.5159e-01, 6.3889e-01],\n",
      "          [6.2029e-01, 1.0423e-01, 4.9382e-01, 5.9580e-01, 6.7167e-01],\n",
      "          [4.2434e-01, 4.9524e-01, 4.1887e-01, 7.0469e-01, 8.4062e-01],\n",
      "          [2.3319e-01, 9.6746e-01, 4.7422e-01, 4.4719e-01, 4.0477e-01]],\n",
      "\n",
      "         [[2.6443e-01, 5.1021e-01, 8.9577e-01, 4.3075e-01, 6.2700e-01],\n",
      "          [6.7484e-01, 7.6510e-01, 3.5344e-01, 1.3965e-01, 2.9886e-01],\n",
      "          [4.1478e-01, 1.2800e-01, 2.9914e-01, 3.2970e-01, 4.7264e-01],\n",
      "          [8.4007e-01, 6.8737e-01, 9.4053e-02, 1.8555e-01, 6.3290e-01],\n",
      "          [7.8415e-01, 1.0784e-01, 8.7546e-01, 9.2015e-01, 5.7597e-01]],\n",
      "\n",
      "         [[7.9618e-01, 5.3000e-02, 2.3244e-01, 5.6909e-01, 3.9514e-01],\n",
      "          [8.7152e-01, 6.9278e-01, 9.0687e-01, 2.3178e-01, 9.4656e-01],\n",
      "          [5.5009e-01, 5.0144e-01, 6.1861e-01, 7.2308e-01, 4.1214e-01],\n",
      "          [7.4491e-01, 1.1176e-01, 3.8687e-01, 1.5520e-01, 3.3221e-01],\n",
      "          [2.3710e-01, 4.8989e-01, 1.4269e-01, 4.4793e-01, 4.3374e-01]],\n",
      "\n",
      "         [[6.2850e-01, 1.2495e-01, 6.6466e-01, 5.9967e-02, 5.3021e-01],\n",
      "          [1.2920e-01, 1.8402e-01, 7.6147e-01, 7.1295e-01, 5.7775e-02],\n",
      "          [5.9152e-01, 1.0980e-01, 5.6079e-01, 3.6310e-01, 2.3139e-01],\n",
      "          [1.2628e-01, 1.1376e-01, 2.6888e-01, 1.6541e-02, 1.7377e-01],\n",
      "          [6.8263e-01, 9.8873e-02, 7.6091e-01, 3.5944e-01, 2.2046e-01]],\n",
      "\n",
      "         [[7.9117e-01, 9.0646e-01, 1.7366e-01, 1.9620e-01, 2.6888e-02],\n",
      "          [8.6061e-01, 9.9954e-01, 2.1191e-01, 8.0137e-01, 9.9727e-01],\n",
      "          [7.7394e-01, 2.2433e-01, 7.0667e-01, 4.6949e-01, 7.1326e-01],\n",
      "          [8.7758e-01, 6.7705e-01, 6.5002e-01, 3.8332e-01, 6.7552e-02],\n",
      "          [3.8186e-01, 5.4430e-01, 6.7497e-01, 6.4977e-01, 9.1681e-02]]],\n",
      "\n",
      "\n",
      "        [[[5.4271e-02, 3.2587e-01, 2.9104e-01, 5.6889e-01, 6.0214e-01],\n",
      "          [3.8512e-02, 4.4184e-01, 8.3103e-01, 6.9073e-02, 1.7650e-01],\n",
      "          [1.9718e-01, 8.4710e-01, 1.6494e-01, 3.7743e-01, 2.1605e-01],\n",
      "          [7.7089e-01, 9.8727e-01, 7.4404e-01, 2.2929e-01, 2.1420e-01],\n",
      "          [4.8923e-02, 8.4400e-01, 3.0606e-01, 1.0693e-02, 9.6898e-01]],\n",
      "\n",
      "         [[8.1062e-01, 7.5880e-01, 7.0624e-02, 6.1873e-01, 2.8730e-01],\n",
      "          [7.1093e-01, 1.7513e-01, 2.2410e-01, 3.7633e-01, 7.1973e-01],\n",
      "          [3.4577e-02, 5.4357e-01, 3.5170e-01, 2.9344e-01, 2.3902e-01],\n",
      "          [2.7878e-01, 7.0052e-01, 8.2733e-01, 7.8156e-01, 9.4858e-01],\n",
      "          [2.6176e-01, 9.3713e-01, 3.2303e-01, 9.6858e-01, 8.2589e-01]],\n",
      "\n",
      "         [[1.1750e-01, 5.8185e-01, 2.6914e-01, 4.2687e-01, 2.1775e-01],\n",
      "          [8.9839e-01, 6.0316e-01, 2.5487e-01, 8.1197e-01, 8.2952e-01],\n",
      "          [6.2234e-01, 8.0618e-01, 8.7516e-01, 8.9123e-01, 3.2184e-01],\n",
      "          [7.4138e-01, 5.2947e-02, 4.8304e-01, 9.5386e-01, 6.1180e-01],\n",
      "          [8.1588e-01, 4.2503e-01, 2.9812e-01, 3.9392e-01, 5.4141e-02]],\n",
      "\n",
      "         [[4.3892e-01, 7.1802e-01, 6.5253e-01, 7.2558e-01, 5.6625e-01],\n",
      "          [3.2172e-01, 8.6878e-01, 9.9822e-01, 6.6990e-01, 6.5268e-01],\n",
      "          [4.7397e-01, 2.6111e-01, 7.1843e-01, 2.2436e-01, 7.1580e-01],\n",
      "          [5.6395e-01, 2.0382e-01, 6.3129e-01, 1.6443e-02, 6.4470e-01],\n",
      "          [1.5885e-01, 2.2629e-01, 8.9578e-01, 5.0378e-01, 2.9662e-02]],\n",
      "\n",
      "         [[9.6599e-01, 1.2865e-01, 6.2244e-02, 6.1433e-01, 4.4633e-01],\n",
      "          [3.3392e-01, 6.1354e-01, 5.1022e-01, 2.8972e-01, 5.1542e-01],\n",
      "          [9.7539e-01, 3.4583e-01, 1.3909e-01, 9.2532e-02, 3.0434e-01],\n",
      "          [6.0570e-02, 9.1880e-01, 8.5147e-01, 3.6662e-01, 2.2759e-01],\n",
      "          [8.5051e-01, 2.7882e-01, 2.6974e-01, 4.6114e-01, 2.1285e-01]],\n",
      "\n",
      "         [[7.1936e-02, 8.4121e-01, 3.8986e-01, 7.3818e-01, 9.8179e-01],\n",
      "          [8.1365e-01, 6.8113e-01, 6.6251e-01, 4.3083e-01, 3.5374e-01],\n",
      "          [2.7426e-01, 6.9832e-01, 3.5264e-01, 8.3014e-01, 1.8152e-01],\n",
      "          [1.5750e-01, 4.2705e-02, 5.7826e-01, 4.9092e-01, 5.3605e-01],\n",
      "          [1.6437e-02, 5.5007e-01, 9.2903e-01, 6.1004e-01, 9.5243e-01]]],\n",
      "\n",
      "\n",
      "        [[[3.1686e-01, 6.4050e-01, 9.7183e-01, 7.9043e-01, 9.7344e-02],\n",
      "          [8.9363e-01, 7.6707e-01, 5.7278e-02, 9.5246e-01, 1.3945e-01],\n",
      "          [5.8214e-01, 8.8186e-01, 1.6423e-01, 2.1397e-01, 7.6852e-01],\n",
      "          [4.0155e-01, 2.7191e-01, 9.0973e-01, 7.8725e-01, 1.0882e-01],\n",
      "          [5.4053e-01, 3.0020e-01, 6.9633e-01, 9.5818e-01, 7.7063e-04]],\n",
      "\n",
      "         [[3.6693e-01, 8.3787e-01, 5.8080e-01, 1.3851e-01, 6.3376e-01],\n",
      "          [8.9725e-01, 9.9393e-01, 8.6775e-01, 3.3476e-01, 1.5475e-01],\n",
      "          [1.4923e-01, 4.1025e-02, 5.6069e-01, 7.7847e-01, 3.7435e-01],\n",
      "          [2.8005e-01, 4.5859e-01, 6.4755e-01, 3.2348e-01, 8.3317e-01],\n",
      "          [7.9447e-01, 8.5049e-01, 5.0493e-01, 3.6193e-01, 1.4408e-01]],\n",
      "\n",
      "         [[4.7693e-01, 6.5524e-01, 9.1828e-01, 7.6024e-01, 4.2601e-01],\n",
      "          [5.1650e-01, 8.9057e-01, 2.7027e-01, 1.9122e-01, 5.3116e-01],\n",
      "          [3.0309e-01, 5.5442e-01, 9.1984e-01, 4.0574e-01, 7.7684e-01],\n",
      "          [4.5638e-01, 1.3851e-01, 5.6153e-01, 4.0532e-01, 4.2331e-01],\n",
      "          [4.3175e-01, 1.6480e-01, 4.8031e-01, 6.1347e-01, 5.4649e-01]],\n",
      "\n",
      "         [[8.7831e-01, 8.5395e-01, 1.3349e-01, 8.8237e-01, 8.1205e-02],\n",
      "          [2.6692e-02, 3.0183e-01, 6.4373e-01, 8.7772e-01, 7.5265e-01],\n",
      "          [7.5112e-01, 8.3880e-01, 2.4632e-01, 3.6357e-01, 2.6744e-02],\n",
      "          [3.6195e-01, 3.8512e-01, 1.3042e-01, 4.9457e-01, 1.5963e-01],\n",
      "          [1.0053e-01, 6.2479e-01, 3.2371e-01, 3.2591e-01, 6.7698e-01]],\n",
      "\n",
      "         [[4.9459e-01, 6.2184e-01, 2.0216e-01, 3.5044e-01, 2.1612e-01],\n",
      "          [8.5526e-02, 7.8765e-01, 4.0221e-03, 6.3011e-01, 4.9106e-01],\n",
      "          [5.5386e-02, 7.8139e-01, 3.2706e-01, 5.1338e-01, 8.5166e-01],\n",
      "          [9.8713e-01, 5.5876e-01, 2.8257e-01, 1.8979e-01, 5.3033e-01],\n",
      "          [1.7638e-01, 8.7412e-01, 2.7739e-01, 7.0138e-01, 3.1082e-03]],\n",
      "\n",
      "         [[9.7842e-03, 1.8521e-01, 7.2538e-01, 5.5248e-01, 6.4440e-01],\n",
      "          [2.1606e-01, 2.7060e-01, 9.2053e-01, 1.1877e-01, 8.8464e-01],\n",
      "          [8.0960e-01, 9.4248e-01, 9.8999e-01, 5.6971e-01, 8.2658e-01],\n",
      "          [8.3595e-01, 1.2387e-01, 6.8004e-01, 2.2933e-01, 2.0338e-01],\n",
      "          [6.1143e-01, 1.7088e-01, 4.1282e-01, 9.9224e-01, 5.0001e-01]]]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0230,  0.0338, -0.0563,  0.0695, -0.0585,  0.0739,  0.0710, -0.0191,\n",
      "        -0.0172, -0.0284, -0.0376, -0.0544,  0.0522, -0.0671,  0.0580, -0.0710],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0143, -0.0337,  0.0097,  ...,  0.0333,  0.0452, -0.0407],\n",
      "        [ 0.0270, -0.0040,  0.0413,  ..., -0.0105, -0.0320,  0.0284],\n",
      "        [ 0.0301, -0.0299, -0.0117,  ...,  0.0052, -0.0081,  0.0048],\n",
      "        ...,\n",
      "        [ 0.0066, -0.0264, -0.0307,  ..., -0.0347,  0.0130, -0.0163],\n",
      "        [-0.0225,  0.0378, -0.0488,  ..., -0.0389, -0.0067,  0.0471],\n",
      "        [ 0.0474, -0.0178,  0.0153,  ..., -0.0003, -0.0078, -0.0457]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0204, -0.0479, -0.0211,  0.0144, -0.0336, -0.0372,  0.0345, -0.0492,\n",
      "         0.0428, -0.0294,  0.0065, -0.0497,  0.0363, -0.0246, -0.0052,  0.0384,\n",
      "        -0.0217,  0.0103,  0.0251,  0.0112, -0.0418,  0.0332, -0.0489,  0.0053,\n",
      "         0.0134,  0.0134,  0.0263, -0.0131,  0.0256,  0.0048,  0.0076,  0.0168,\n",
      "        -0.0413,  0.0204, -0.0241, -0.0480,  0.0408, -0.0337,  0.0260, -0.0310,\n",
      "        -0.0429,  0.0437,  0.0182, -0.0051, -0.0387, -0.0481,  0.0088, -0.0037,\n",
      "        -0.0238,  0.0153,  0.0010,  0.0329,  0.0196, -0.0449, -0.0433,  0.0042,\n",
      "         0.0455, -0.0279,  0.0181, -0.0395, -0.0160, -0.0020, -0.0096, -0.0116,\n",
      "        -0.0022, -0.0449, -0.0144, -0.0394, -0.0096,  0.0300,  0.0029, -0.0047,\n",
      "         0.0403, -0.0490, -0.0134, -0.0002,  0.0362, -0.0310, -0.0032, -0.0284,\n",
      "         0.0138,  0.0317,  0.0460,  0.0431, -0.0381,  0.0322, -0.0003,  0.0162,\n",
      "        -0.0097, -0.0144,  0.0278, -0.0109,  0.0258,  0.0495,  0.0442,  0.0480,\n",
      "         0.0050, -0.0445,  0.0325, -0.0047, -0.0223, -0.0199,  0.0443, -0.0325,\n",
      "        -0.0413, -0.0266,  0.0320,  0.0327,  0.0345, -0.0253,  0.0042, -0.0025,\n",
      "        -0.0466,  0.0170, -0.0130, -0.0282, -0.0055, -0.0139, -0.0309,  0.0055],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0746, -0.0118, -0.0393,  ...,  0.0631, -0.0324, -0.0043],\n",
      "        [ 0.0610,  0.0201, -0.0353,  ..., -0.0436,  0.0813,  0.0380],\n",
      "        [-0.0116, -0.0252,  0.0386,  ..., -0.0127, -0.0721,  0.0554],\n",
      "        ...,\n",
      "        [ 0.0743,  0.0169, -0.0371,  ...,  0.0502, -0.0908, -0.0452],\n",
      "        [ 0.0446, -0.0053,  0.0719,  ...,  0.0588, -0.0281,  0.0777],\n",
      "        [-0.0624, -0.0390, -0.0137,  ..., -0.0688, -0.0385,  0.0090]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0044,  0.0182, -0.0105,  0.0389, -0.0677,  0.0244,  0.0316, -0.0356,\n",
      "        -0.0866,  0.0562, -0.0645, -0.0834,  0.0761, -0.0563, -0.0680,  0.0529,\n",
      "        -0.0103, -0.0297,  0.0739, -0.0453,  0.0474,  0.0248, -0.0318, -0.0027,\n",
      "         0.0378,  0.0587,  0.0855,  0.0639, -0.0053, -0.0586, -0.0407,  0.0139,\n",
      "         0.0575, -0.0729, -0.0766,  0.0065, -0.0429, -0.0094,  0.0860, -0.0393,\n",
      "         0.0739,  0.0598, -0.0177, -0.0354,  0.0464, -0.0052,  0.0614,  0.0806,\n",
      "         0.0069, -0.0843,  0.0902,  0.0894, -0.0566,  0.0377, -0.0199, -0.0586,\n",
      "         0.0287,  0.0252,  0.0625,  0.0603,  0.0256,  0.0798, -0.0292, -0.0909,\n",
      "        -0.0562,  0.0714,  0.0025,  0.0721, -0.0794, -0.0129, -0.0015,  0.0806,\n",
      "        -0.0651, -0.0419, -0.0623,  0.0150, -0.0044,  0.0423, -0.0343, -0.0223,\n",
      "        -0.0666, -0.0460,  0.0194,  0.0345], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0823, -0.0643, -0.0812,  0.0983,  0.0081,  0.0180, -0.0498, -0.0109,\n",
      "         -0.0511, -0.0567,  0.0095, -0.0718, -0.0229, -0.0592,  0.0696,  0.0008,\n",
      "         -0.0264,  0.0767,  0.0869, -0.0024, -0.1002,  0.0993, -0.0735, -0.0510,\n",
      "          0.0675,  0.0695,  0.0592, -0.0939,  0.0496,  0.0631, -0.0628, -0.0466,\n",
      "          0.0848,  0.0359,  0.0944, -0.0569,  0.0554,  0.0517, -0.0661, -0.0337,\n",
      "         -0.1035,  0.0420, -0.0989,  0.0842,  0.0120, -0.0638, -0.1066,  0.0456,\n",
      "         -0.0150,  0.1034, -0.0855, -0.0110, -0.0813, -0.0855,  0.0913, -0.0282,\n",
      "         -0.0034,  0.0046,  0.0032,  0.0222, -0.0935, -0.0978, -0.0381,  0.0708,\n",
      "          0.0192,  0.0952,  0.0126,  0.0556, -0.0009, -0.0048,  0.0977, -0.0495,\n",
      "          0.0913,  0.0135, -0.0928,  0.0428, -0.0829, -0.0063,  0.0261, -0.0862,\n",
      "         -0.0127, -0.0590,  0.0969,  0.0753],\n",
      "        [-0.0115, -0.0553, -0.0355,  0.0504,  0.0933, -0.0955,  0.0184, -0.0210,\n",
      "         -0.0095,  0.0994, -0.0606,  0.1048,  0.0705, -0.0108, -0.0177, -0.0213,\n",
      "         -0.1041,  0.1023,  0.0554,  0.0003,  0.0524,  0.0720, -0.0947, -0.0875,\n",
      "         -0.0568,  0.0266, -0.0377,  0.0911, -0.0694,  0.0174, -0.0097,  0.0332,\n",
      "          0.0201,  0.1006,  0.0953,  0.0906,  0.0483,  0.0563,  0.0036, -0.0066,\n",
      "          0.0776, -0.0861,  0.0963,  0.0795, -0.1058, -0.0013, -0.0946, -0.0111,\n",
      "         -0.0684, -0.0846, -0.0475,  0.0236, -0.0187,  0.0092,  0.0428, -0.0868,\n",
      "         -0.0671,  0.0363, -0.0550, -0.0920,  0.0754,  0.0518, -0.0036,  0.0140,\n",
      "          0.0852,  0.0771,  0.0643,  0.0730,  0.0310,  0.0759,  0.0633,  0.0592,\n",
      "          0.0694,  0.0245, -0.0689,  0.0537, -0.0110,  0.0432, -0.0177,  0.0752,\n",
      "         -0.0694,  0.0906, -0.0020,  0.0704],\n",
      "        [ 0.0936,  0.0554,  0.0342, -0.0180,  0.0576, -0.0623, -0.0276, -0.0971,\n",
      "         -0.0531, -0.0239,  0.0303,  0.0308, -0.0888,  0.0588, -0.0924,  0.0218,\n",
      "          0.0234, -0.0536,  0.1018,  0.0377, -0.0941, -0.0898,  0.0119, -0.0417,\n",
      "          0.0261, -0.0284,  0.0999, -0.0287,  0.0323,  0.0381, -0.0006,  0.0519,\n",
      "         -0.0843, -0.0809, -0.0562, -0.0817, -0.0828, -0.0711, -0.0246,  0.0841,\n",
      "         -0.0088, -0.0806, -0.0788,  0.1032, -0.0261, -0.0965,  0.0648,  0.0083,\n",
      "          0.0348,  0.0163, -0.0221,  0.0806, -0.0664,  0.0854, -0.0530, -0.0703,\n",
      "          0.0676,  0.0790,  0.0487,  0.0703, -0.0340,  0.0656,  0.0094,  0.0205,\n",
      "         -0.0825, -0.0118, -0.0978, -0.0384, -0.0826,  0.0191,  0.0788,  0.0117,\n",
      "          0.0470, -0.0889,  0.0614,  0.0179, -0.0356,  0.0631, -0.0379, -0.0398,\n",
      "         -0.0706, -0.0153,  0.0138,  0.0628],\n",
      "        [-0.0888,  0.0100,  0.0560,  0.0924, -0.0859,  0.0937, -0.0795,  0.0234,\n",
      "          0.0397, -0.0269, -0.0884, -0.0963, -0.0193,  0.0191,  0.0548,  0.0518,\n",
      "          0.0827, -0.0866,  0.0719, -0.0834,  0.0275,  0.0726,  0.0767, -0.0859,\n",
      "         -0.0850,  0.0495, -0.0505,  0.0489,  0.0300, -0.0710,  0.0831,  0.0090,\n",
      "          0.0818, -0.0286, -0.1011,  0.1036,  0.0789, -0.0389, -0.0601,  0.1016,\n",
      "         -0.0635, -0.0165,  0.0189,  0.0892,  0.0324, -0.1017,  0.0034, -0.0763,\n",
      "          0.0589,  0.0945, -0.0553,  0.1081,  0.0408,  0.0896,  0.0565, -0.0200,\n",
      "         -0.0915, -0.0102,  0.1089, -0.1052, -0.0940, -0.0976,  0.0924, -0.0984,\n",
      "          0.0798, -0.0289, -0.0432,  0.0331, -0.0908, -0.0067,  0.0520, -0.0364,\n",
      "          0.0749,  0.0212, -0.0348, -0.0036, -0.0698,  0.0382, -0.0674, -0.0913,\n",
      "          0.0194, -0.0598, -0.0509, -0.0165],\n",
      "        [ 0.0286,  0.0535,  0.0724,  0.1000, -0.0175, -0.0897, -0.0355,  0.0990,\n",
      "         -0.0203,  0.0388,  0.0538, -0.1039, -0.0076, -0.0161, -0.0208, -0.0418,\n",
      "          0.0009,  0.1076,  0.0458, -0.0622,  0.0282,  0.0023,  0.0427,  0.0719,\n",
      "         -0.0246, -0.0199, -0.1007,  0.0859,  0.0268,  0.0361,  0.0856,  0.0784,\n",
      "          0.0740, -0.0600,  0.0799,  0.0527,  0.1047,  0.0488,  0.0339,  0.0174,\n",
      "          0.0725, -0.0799,  0.0850,  0.0350,  0.0697,  0.0949, -0.0249, -0.0012,\n",
      "         -0.0287, -0.0305, -0.0983,  0.0655,  0.0918,  0.0101,  0.1091,  0.0495,\n",
      "         -0.0559, -0.0608, -0.0167,  0.0765,  0.0824, -0.0928,  0.0547, -0.0143,\n",
      "         -0.0151, -0.0420,  0.0181,  0.1014, -0.0710,  0.0790,  0.0342,  0.0734,\n",
      "          0.1051,  0.0404,  0.0243, -0.0782,  0.0132, -0.0996, -0.0425, -0.0038,\n",
      "         -0.0875,  0.0650,  0.0314,  0.0744],\n",
      "        [-0.0882,  0.0814,  0.0298, -0.0801, -0.0595, -0.0861,  0.0011,  0.0350,\n",
      "         -0.0949,  0.0398, -0.0457, -0.0082,  0.0959,  0.0268,  0.0946, -0.0109,\n",
      "          0.0879, -0.0694,  0.1083, -0.0123,  0.0634,  0.0449,  0.0116, -0.1049,\n",
      "          0.0116,  0.0579, -0.0802, -0.0797, -0.0177,  0.1017,  0.0622, -0.0876,\n",
      "          0.0876,  0.0487, -0.0643, -0.0290,  0.0163,  0.0360,  0.0530,  0.0090,\n",
      "          0.0639,  0.0932,  0.0533,  0.0067, -0.0548, -0.0745,  0.0703, -0.0077,\n",
      "          0.0624,  0.0297,  0.0463,  0.0363, -0.0392, -0.0391, -0.1056,  0.0074,\n",
      "          0.1069, -0.0846, -0.0063, -0.1051, -0.0584,  0.0673,  0.0564,  0.0098,\n",
      "          0.0438,  0.0947,  0.1084,  0.0512, -0.0455, -0.0628, -0.0641, -0.0054,\n",
      "         -0.0869,  0.0051,  0.0087, -0.0804,  0.0843,  0.0513,  0.0972,  0.1011,\n",
      "         -0.0072,  0.0464, -0.0808, -0.0994],\n",
      "        [-0.0903, -0.0198, -0.0681,  0.0702, -0.0363,  0.0463,  0.0481,  0.0447,\n",
      "         -0.0444,  0.0351, -0.0228,  0.0038, -0.0738, -0.0426,  0.0470,  0.0118,\n",
      "          0.0527, -0.0176, -0.0680,  0.0065,  0.0823,  0.0741, -0.0725,  0.1087,\n",
      "          0.0568,  0.0842,  0.0599, -0.0485, -0.0917,  0.1006,  0.0418,  0.0173,\n",
      "          0.0335,  0.0602,  0.0125,  0.1053, -0.0996,  0.0312, -0.0308,  0.0658,\n",
      "         -0.0296,  0.0303,  0.0548, -0.0504, -0.0565,  0.0620, -0.1056,  0.0938,\n",
      "         -0.0444,  0.0334,  0.0973,  0.0012, -0.0095,  0.1065,  0.0935, -0.0053,\n",
      "          0.0802, -0.0093,  0.0485, -0.0833, -0.0592, -0.1008,  0.1025, -0.0260,\n",
      "          0.0774, -0.0490, -0.0640,  0.0405, -0.1065, -0.0616, -0.0866,  0.0628,\n",
      "         -0.0448,  0.0493,  0.0297, -0.0196,  0.0901, -0.0523,  0.0926, -0.0122,\n",
      "          0.0922, -0.0359, -0.0264,  0.0525],\n",
      "        [-0.0864,  0.0954, -0.0028,  0.0293, -0.0270,  0.0389,  0.0060, -0.1026,\n",
      "         -0.0637, -0.0651, -0.0629, -0.0287,  0.0828, -0.0917,  0.0810,  0.0648,\n",
      "          0.0125,  0.0500, -0.0360, -0.1014,  0.0044,  0.0574, -0.0167,  0.0131,\n",
      "          0.0159, -0.0139, -0.1083,  0.1005, -0.0133, -0.0975, -0.0898, -0.0909,\n",
      "         -0.0128,  0.0347,  0.0133,  0.0672, -0.0208,  0.0382, -0.0863, -0.0341,\n",
      "          0.1073, -0.0913,  0.0025, -0.0163,  0.0940, -0.0233,  0.0712,  0.0310,\n",
      "         -0.0168, -0.0143, -0.0541,  0.0868,  0.0888, -0.1046, -0.0124, -0.0315,\n",
      "          0.0154, -0.0892, -0.0062, -0.0570,  0.0742, -0.0204, -0.1005, -0.0059,\n",
      "         -0.0145, -0.1030, -0.0066,  0.0238,  0.0797, -0.0675, -0.1052, -0.0969,\n",
      "          0.0973, -0.1027,  0.0086, -0.0209,  0.0059, -0.0653, -0.0797, -0.0287,\n",
      "         -0.0372,  0.0657, -0.0212,  0.0781],\n",
      "        [-0.0362, -0.0368,  0.0876, -0.1019,  0.0355, -0.0937,  0.1030, -0.0405,\n",
      "         -0.0716, -0.0201, -0.0479,  0.1024, -0.0900,  0.0785,  0.0755, -0.0306,\n",
      "          0.0337, -0.0441,  0.1088,  0.0906, -0.1057,  0.1018,  0.0103, -0.0769,\n",
      "          0.0957,  0.0216, -0.0805,  0.0567, -0.1033, -0.0680, -0.0361,  0.0615,\n",
      "         -0.0190,  0.0893,  0.0323,  0.0443,  0.1060,  0.0550,  0.1087, -0.0530,\n",
      "          0.0464, -0.0978,  0.0885, -0.0825, -0.0185, -0.0837, -0.0733,  0.0148,\n",
      "         -0.0448,  0.0175, -0.0308, -0.0678,  0.0567,  0.0755,  0.0861,  0.0179,\n",
      "         -0.0424, -0.0364, -0.0689,  0.0790,  0.0948, -0.0844, -0.0842,  0.0893,\n",
      "         -0.0292,  0.0493,  0.0969, -0.0452, -0.0046,  0.0715,  0.0514,  0.0466,\n",
      "         -0.0629,  0.0054, -0.0521,  0.0470, -0.0431, -0.0082, -0.0808, -0.0305,\n",
      "          0.0806, -0.0886, -0.0623,  0.0032],\n",
      "        [ 0.0391,  0.0301, -0.0786, -0.0549,  0.0544,  0.0604,  0.0020,  0.0554,\n",
      "          0.0642,  0.0026,  0.0863, -0.0498, -0.0121,  0.0939,  0.0847,  0.0026,\n",
      "          0.0234, -0.0682,  0.0034, -0.0136, -0.1027,  0.1066,  0.0683,  0.0145,\n",
      "          0.0621, -0.0045,  0.0049,  0.0916,  0.0964, -0.0484,  0.0324,  0.0921,\n",
      "         -0.0489,  0.0754, -0.0805,  0.0462, -0.0058,  0.0030,  0.0472, -0.1036,\n",
      "          0.0267, -0.0695, -0.1013,  0.0835, -0.0024,  0.0482, -0.0802,  0.0056,\n",
      "          0.0628,  0.0008,  0.1035, -0.0176, -0.0117, -0.1069,  0.0379, -0.0330,\n",
      "         -0.0077,  0.0076,  0.0609, -0.0774,  0.0125, -0.0082,  0.0884,  0.0232,\n",
      "          0.0203, -0.1013, -0.0484,  0.0481,  0.0775,  0.0540, -0.0687,  0.0681,\n",
      "         -0.0112,  0.0519, -0.0249, -0.0913,  0.0852,  0.0284, -0.0579,  0.0690,\n",
      "         -0.0114, -0.0320, -0.0074,  0.0760]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0576, -0.0942, -0.0832, -0.0550, -0.0957, -0.0138, -0.1082,  0.0409,\n",
      "         0.0138,  0.0921], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "\n",
    "for i in params:\n",
    "    print(i.shape)\n",
    "    \n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try a random 32x32 input\n",
    "Note: Expected input size to this net(LeNet) is 32x32. To use this net on\n",
    "MNIST dataset, please resize the images from the dataset to 32x32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.9075, -0.3262,  1.6211,  ...,  0.4470,  0.7173, -0.9660],\n",
      "          [ 1.1712, -1.3710,  0.1153,  ..., -1.6503, -0.4119, -0.1123],\n",
      "          [-0.2234, -1.1084, -0.3526,  ...,  0.6143,  0.7884,  1.1465],\n",
      "          ...,\n",
      "          [-0.6864, -0.1405,  1.6630,  ..., -0.4787,  0.2018, -1.2964],\n",
      "          [ 0.9528, -0.3871,  0.3279,  ...,  1.7516,  1.2347,  0.2796],\n",
      "          [ 2.4674, -0.3897,  0.3117,  ...,  1.6496, -0.8743, -1.9957]]]])\n",
      "tensor([[ 10.1932,  16.7110, -26.8263,   8.5804,  25.5562, -25.3286,  47.9618,\n",
      "          -0.6238, -31.0383,   6.4824]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "print(input)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random\n",
    "gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.1890, -1.7642, -1.2407, -0.6905,  1.4818],\n",
       "          [-3.0232,  0.5179,  0.0531, -1.8498, -0.0746],\n",
       "          [-2.7031,  1.8405, -0.8986, -1.1680, -4.4293],\n",
       "          [-3.6899, -1.5791,  1.8281,  0.8276, -1.3301],\n",
       "          [-1.0217, -0.1033, -1.8131,  1.6611, -2.1874]]],\n",
       "\n",
       "\n",
       "        [[[-1.5841,  0.8583,  0.3118, -0.7311, -1.8117],\n",
       "          [-2.0638,  0.2793,  0.9307, -1.9093,  0.4375],\n",
       "          [-3.1827, -0.7752, -1.2239, -1.7563, -1.4826],\n",
       "          [-1.7069,  0.2144,  1.3982, -1.0978,  0.7146],\n",
       "          [-3.2961,  0.0650,  1.5228, -2.4158, -3.0712]]],\n",
       "\n",
       "\n",
       "        [[[-1.7227, -1.0544, -0.0397, -2.7508, -1.0612],\n",
       "          [-1.9456, -0.0552, -0.9607, -0.9534, -1.5147],\n",
       "          [ 0.0090,  1.1706, -0.7672, -0.6291, -1.0155],\n",
       "          [-1.5231, -1.6951, -1.2303, -0.9434,  1.1715],\n",
       "          [ 0.4837,  2.1094,  0.6831, -3.6687, -1.9772]]],\n",
       "\n",
       "\n",
       "        [[[-3.3978, -1.9479,  1.2797, -1.8634,  0.7480],\n",
       "          [-0.7653,  2.2482,  0.6680, -1.6831, -1.5829],\n",
       "          [-2.1136, -1.9682, -0.0748, -1.0898, -0.3449],\n",
       "          [-2.7963, -1.9085,  0.0890,  0.6091, -1.0673],\n",
       "          [-2.2455, -0.1432,  1.0209, -0.5397, -3.4192]]],\n",
       "\n",
       "\n",
       "        [[[-2.8514, -0.3506, -1.4595, -0.5088, -1.6340],\n",
       "          [ 0.0403,  0.3839, -0.2516, -2.1615, -0.7102],\n",
       "          [-2.0098,  0.4697,  0.0656, -2.7123, -1.2878],\n",
       "          [-0.0654,  0.3734,  1.5415, -1.7607, -1.6297],\n",
       "          [-1.6087, -2.4895, -0.8137, -1.8998,  0.6169]]],\n",
       "\n",
       "\n",
       "        [[[-0.5115, -0.1205, -1.5832,  0.0388, -0.2733],\n",
       "          [-3.5593, -2.4485,  1.3568, -2.3425, -0.7826],\n",
       "          [-1.2103,  0.3811,  0.8870,  0.5826, -2.6892],\n",
       "          [-2.2346,  0.3973,  0.8139, -1.6112, -0.5451],\n",
       "          [-1.8703, -1.2525, -1.8632, -1.0919, -3.8196]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "params[0].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
    "    package only supports inputs that are a mini-batch of samples, and not\n",
    "    a single sample.\n",
    "\n",
    "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "    ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "    a fake batch dimension.</p></div>\n",
    "\n",
    "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
    "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
    "     tensor.\n",
    "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Tensor`` operation, creates at\n",
    "     least a single ``Function`` node, that connects to functions that\n",
    "     created a ``Tensor`` and *encodes its history*.\n",
    "\n",
    "**At this point, we covered:**\n",
    "  -  Defining a neural network\n",
    "  -  Processing inputs and calling backward\n",
    "\n",
    "**Still Left:**\n",
    "  -  Computing the loss\n",
    "  -  Updating the weights of the network\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different\n",
    "`loss functions <http://pytorch.org/docs/nn.html#loss-functions>`_ under the\n",
    "nn package .\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
    "between the input and the target.\n",
    "\n",
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6427, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.arange(1, 11)  # a dummy target, for example\n",
    "target = target.view(1, -1).float()  # make it the same shape as output\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow ``loss`` in the backward direction, using its\n",
    "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
    "like this:\n",
    "\n",
    "::\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
    "w.r.t. the loss, and all Tensors in the graph that has ``requres_grad=True``\n",
    "will have their ``.grad`` Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BinaryCrossEntropyWithLogitsBackward object at 0x000001FF312D8148>\n",
      "<AddmmBackward object at 0x000001FF312D8148>\n",
      "<AccumulateGrad object at 0x000001FF312E9CC8>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([0.4392, 0.1056, 0.4390, 0.7510, 0.1132, 0.2902])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is `here <http://pytorch.org/docs/nn>`_.\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "\n",
    "  - Updating the weights of the network\n",
    "\n",
    "Update the weights\n",
    "------------------\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):\n",
    "\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "We can implement this using simple python code:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "# optimizer = optim.RMSProp(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Note::\n",
    "\n",
    "      Observe how gradient buffers had to be manually set to zero using\n",
    "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
    "      as explained in `Backprop`_ section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and Loading Model\n",
    "------------------\n",
    "\n",
    "**Saving**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './model1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load('./model1.pt'))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
